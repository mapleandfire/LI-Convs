
[data]
dataset: celeba_mask   ; dataset name
dataset_dir: ../../dataset/CelebAMask-HQ/tfrecord   ; directory path containing the TFRecord files
train_split: train
val_split: val
test_split: test
snapshot_root: ../../LI-Convs-snapshots/celeba_mask   ; snapshot root to save the experimental data

[model]
model_variant: li_resnet_v1_50_beta  ; the structure for the backbone network
dpn_variant: li_aspp  ; the decoding module, can be aspp or li_aspp
decoder_output_stride: 4   ; if this line is commented, no decoder is used; else a decoder with this os will be applied
add_image_level_feature: True  ; use image level pooling in aspp or not
aspp_with_batch_norm: True    ; use dpn or not
multi_grid: 1,2,4   ; set to 1,2,4 for 'resnet_v1_{50,101}_beta' checkpoint

[lateral_inhibition]
li_zones_dpn: 3,3,3   ; li zones for the three LI layers in LI-ASPP
li_decay_stds_dpn: 1.0,1.0,1.0   ; li standard deviation (sigma) for the three LI layers in LI-ASPP
li_rates_dpn: 1,1,1   ; dilation rates for LI layers in LI-ASPP
li_zones_backbone: 3   ; li zone for LI layers in backbone
li_decay_stds_backbone: 1.0  ; li standard deviation (sigma) for LI layers in backbone
li_rates_backbone: 1    ; dilation rates for LI layers in backbone
li_activation: relu
li_weight_initilizer: random_uniform
li_weight_initilizer_min_max_backbone: 0.0,0.0    ; li weights initialization range for LI-based backbones
li_weight_initilizer_min_max_dpn: 0.0,0.0   ; li weights initialization range for LI-ASPP
li_weight_clip_values: 0.0,1.0    ; constrain li weights to be [0.0,1.0]
li_with_batch_norm: False
li_with_weight_regularizer: False
li_resnet_option: d   ; the options to add LI layers in ResNet
li_resnet_bottleneck_pos: after_conv1   ; the options to add LI layers in a ResNet bottleneck
li_resnet_block_pos: last   ; the options to add LI layer in a ResNet block


[train]
tf_initial_checkpoint: ../../LI-Convs-snapshots/init_models/resnet_v1_50_2018_05_04/model.ckpt  ; the path of the pre-trained checkpoint for initialization
freeze_backbone_network: False
fine_tune_batch_norm: True
finetune_li_weight_only: False   ; set True to only finetune LI weights with other weights frozen
num_clones: 2   ; number of available gpus for training
train_batch_size: 16    ; When fine_tune_batch_norm=True, use at least batch size larger than 12 (batch size more than 16 is better)
;li_weight_gradient_multiplier: 1.0    ; The gradient multiplier for lateral inhibition weights
initialize_last_layer: False   ; set False if do not want to restore the trained classifier weights
last_layers_contain_logits_only: False  ; Only valid when initialize_last_layer==False; set False to only restore backbone weights, set True to restore backbone plus other components' (decoder, ASPP, etc.) weights
last_layer_gradient_multiplier: 1.0    ; The gradient multiplier for last layers, set value > 1 to boost training
atrous_rates: 6,12,18   ; comment this line to set atrous_rates=None (disable dilated conv in aspp)
output_stride: 16
training_number_of_steps: 362880
train_crop_size: 513,513  ; input size of training images/labels
base_learning_rate: 0.01   ; Use 0.007 when training on PASCAL augmented training set, train_aug.
learning_rate_decay_factor: 0.1
learning_rate_decay_step: 8000
learning_power: 0.9
momentum: 0.9
weight_decay: 0.0001   ; 0.00004 for MobileNet-V2 or Xcpetion, 0.0001 for ResNet
min_scale_factor: 0.5
max_scale_factor: 2.0
scale_factor_step_size: 0.25
save_checkpoint_steps: 1512  ; interval steps to save checkpoint
save_summaries_steps: 1512   ; interval steps to save summaries
max_to_keep: 160    ; maximum checkpoints to save


[val]
eval_crop_size: 513,513
eval_scales: 1.0       ; single-scale or multiple-scale for evaluation
atrous_rates: 6,12,18
output_stride: 16


[test]
eval_crop_size: 513,513
eval_scales: 1.0
atrous_rates: 6,12,18
output_stride: 16

