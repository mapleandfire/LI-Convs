
[data]
dataset: celeba_mask     ; dataset name
dataset_dir: ../../dataset/CelebAMask-HQ/tfrecord    ; directory path containing the TFRecord files
train_split: train
val_split: val
test_split: test
snapshot_root: ../../LI-Convs-snapshots/celeba_mask     ; snapshot root to save the experimental data

[model]
model_variant: li_mobilenet_v2  ; the backbone network
dpn_variant: li_aspp  ; the decoding module, can be aspp or li_aspp
add_image_level_feature: True  ; use image level pooling in aspp or not
aspp_with_batch_norm: True    ; use dpn or not


[lateral_inhibition]
li_zones_dpn: 3,3,3   ; li zones for the three LI layers in LI-ASPP
li_decay_stds_dpn: 1.0,1.0,1.0     ; li standard deviation (sigma) for the three LI layers in LI-ASPP
li_rates_dpn: 1,1,1   ; dilation rates for LI layers in LI-ASPP
li_zones_backbone: 3   ; li zone for LI layers in backbone
li_decay_stds_backbone: 1.0     ; li standard deviation (sigma) for LI layers in backbone
li_rates_backbone: 1     ; dilation rates for LI layers in backbone
li_activation: relu
li_weight_initilizer: random_uniform
li_weight_initilizer_min_max_dpn: 0.0,0.0    ; li weights initialization range for LI-ASPP
li_weight_initilizer_min_max_backbone: 0.0,0.0     ; li weights initialization range for LI-MNV2
li_weight_clip_values: 0.0,1.0        ; constrain li weights to be [0.0,1.0]
li_location_for_expanded_conv: after_expansion   ; where to place the li layer in the expanded conv block, can be 'after_input' or 'after_expansion'
li_backbone_option_mnv2: f   ; the options to construct li_mobinenet_v2 architecture
li_with_batch_norm: False
li_with_weight_regularizer: False


[train]
tf_initial_checkpoint: ../../LI-Convs-snapshots/init_models/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt   ; path to initialization checkpoint
freeze_backbone_network: False
fine_tune_batch_norm: True
num_clones: 1    ; number of available gpus for training
train_batch_size: 16    ; When fine_tune_batch_norm=True, use at least batch size larger than 12 (batch size more than 16 is better)
initialize_last_layer: False   ; set False if do not want to restore the trained classifier weights
last_layers_contain_logits_only: False  ; Only valid when initialize_last_layer==False; set False to only restore backbone weights, set True to restore backbone plus other components' (decoder, ASPP, etc.) weights
last_layer_gradient_multiplier: 1.0    ; The gradient multiplier for last layers, set value > 1 to boost training
atrous_rates: 6,12,18   ; comment this line to set atrous_rates=None (disable dilated conv in aspp)
output_stride: 16
training_number_of_steps: 362880
train_crop_size: 513,513  ; input size of training images/labels
base_learning_rate: 0.01   ; Use 0.007 when training on PASCAL augmented training set, train_aug.
learning_rate_decay_factor: 0.1
learning_rate_decay_step: 8000
learning_power: 0.9
momentum: 0.9
weight_decay: 0.00004   ; 0.00004 for MobileNet-V2 or Xcpetion, 0.0001 for ResNet
min_scale_factor: 0.5
max_scale_factor: 2.0
scale_factor_step_size: 0.25
save_checkpoint_steps: 1512  ; interval steps to save checkpoint
save_summaries_steps: 1512   ; interval steps to save summaries
max_to_keep: 160    ; maximum checkpoints to save


[val]
eval_crop_size: 513,513
eval_scales: 1.0       ; single-scale or multiple-scale for evaluation
atrous_rates: 6,12,18
output_stride: 16


[test]
eval_crop_size: 513,513
eval_scales: 1.0
atrous_rates: 6,12,18
output_stride: 16
;num_plots_to_save: 5    ; number of plots to save for visualizations

