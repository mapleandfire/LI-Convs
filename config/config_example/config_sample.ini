
[data]
dataset: pascal_voc_seg   ; name of the dataset
dataset_dir: ../../dataset/pascal_voc_seg/tfrecord
train_split: train_aug    ; the split for training
val_split: val   ; the split for validation
test_split: val  ; the split for testing
snapshot_root: ../../LI-Convs-snapshots/pascal_voc2012   ; the root folder to save all results like checkpoints

[model]
model_variant: resnet_v1_50_beta  ; the backbone architecture, see 'networks_map' in 'ROOT/li_models/deeplab_feature_extractor.py' for supported variants
dpn_variant: li_aspp  ; architecture for the dense prediction network (DPN) like ASPP, see 'DPN_SCOPE_MAPPING' in 'ROOT/li_models/deeplab_model.py' for supported variants
# decoder_output_stride: 4   ; if this line is commented, no decoder is used; else a decoder with this output stride will be applied
add_image_level_feature: True  ; use image level pooling in aspp or not
aspp_with_batch_norm: True    ; use dpn or not
multi_grid: 1,2,4    ; set to 1,2,4 for 'resnet_v1_{50,101}_beta' checkpoints.


[lateral_inhibition]
li_zones_dpn: 3,3,3    ; the LI zones for each LI-Convs (if any) in DPN network
li_decay_stds_dpn: 1.0,1.0,1.0   ; the LI decaying standard deviations for each LI-Convs (if any) in DPN
li_rates_dpn: 1,1,1  ; the LI rates (dilated rates) for for each LI-Convs (if any) in DPN
li_zones_backbone: 3   ; the LI zones for LI-Convs (if any) in backbone, currently all LI-Convs share the same LI zones
li_decay_stds_backbone: 1.0  ; the LI decaying standard deviations for each LI-Convs (if any) in backbone
li_rates_backbone: 1   ; the LI rates (dilated rates) for for each LI-Convs (if any) in backbone
li_activation: relu  ; activation function in LI-Convs
li_weight_initilizer: random_uniform   ; how LI weights are initilize within a certain range, currently only support random_uniform
li_weight_initilizer_min_max_backbone: 0.0,0.0  ; the (min, max) range to initilize LI weights in backbone
li_weight_initilizer_min_max_dpn: 0.0,0.05   ; the (min, max) range to initilize LI weights in DPN
li_weight_clip_values: 0.0,1.0   ; limit LI weights to be within [0.0,1.0)
li_with_batch_norm: False    ;  whether to use batch norm after LI layer
li_with_weight_regularizer: False    ; whether to add LI weights into L2-regularization
li_resnet_option: d  ; the options to add LI layers in ResNet, see function 'li_resnet_v1_50_beta' in 'ROOT/li_models/li_backbones.py' for supported options
li_resnet_bottleneck_pos: after_conv1  ; the options to add LI layers in a ResNet bottleneck, currently can be 'after_conv1' or 'before_conv1'. see function 'li_resnet_bottleneck' in 'ROOT/li_utils/conv_ops.py' for details
li_resnet_block_pos: last   ; the options to add LI layer in a ResNet block, currently can be 'last' or 'last_two'. see function 'li_resnet_v1_beta_block' in 'ROOT/li_models/li_backbones.py' for details.


[train]
tf_initial_checkpoint: ../../LI-Convs-snapshots/init_models/resnet_v1_50_2018_05_04/model.ckpt  ; the path of the pre-trained checkpoint for initialization
freeze_backbone_network: False   ; freeze the backbone network or not
fine_tune_batch_norm: True  ; fine tune the batch norm layers (True) or freezing them (False)
num_clones: 2    ; number of available GPUs for training, train_batch_size should be divided exactly by this number
train_batch_size: 16    ; When fine_tune_batch_norm=True, use at least batch size larger than 12 (batch size more than 16 is better)
li_weight_gradient_multiplier: 1.0    ; The gradient multiplier for lateral inhibition weights
initialize_last_layer: False   ; set False if do not want to restore the trained classifier weights
last_layers_contain_logits_only: False  ; Only valid when initialize_last_layer==False; set False to only restore backbone weights, set True to restore backbone plus other components' (decoder, ASPP, etc.) weights
last_layer_gradient_multiplier: 1.0    ; The gradient multiplier for last layers, set value > 1 to boost training
atrous_rates: 6,12,18   ; Dilated rates for each dilated convolution in DPN. Comment this line to set atrous_rates=None (disable dilated conv in aspp)
output_stride: 16  ; the output stride
training_number_of_steps: 115440   ; total training steps
train_crop_size: 513,513    ; input size of training images/labels
base_learning_rate: 0.007   ; base learning rate for sgd optimizer with momentum. Use 0.007 when training on PASCAL augmented training set, train_aug.
learning_rate_decay_factor: 0.1    ; the lr decaying factor for sgd optimizer with momentum
learning_rate_decay_step: 8000   ; decay lr after those steps for sgd optimizer with momentum
learning_power: 0.9   ; The power value used in the poly learning policy for sgd optimizer with momentum
momentum: 0.9  ; momentum for sgd optimizer with momentum
optimizer: adam    ; which optimizer to use, adam or sgd
adam_learning_rate: 3e-04   ; initial learning rate for adam
adam_epsilon: 1e-02    ; adam epsilon
weight_decay: 0.0001   ; 0.00004 for MobileNet-V2 or Xcpetion, 0.0001 for ResNet
min_scale_factor: 0.5   ; the minimum scale to resize image during training
max_scale_factor: 2.0   ; the maximum scale to resize image during training
scale_factor_step_size: 0.25
save_checkpoint_steps: 962  ; interval steps to save checkpoint
save_summaries_steps: 962    ; interval steps to save summaries
max_to_keep: 100    ; maximum checkpoints to save


[val]
eval_crop_size: 513,513   ; the image crop size for validation
eval_scales: 1.0       ; single-scale or multiple-scale for validation
atrous_rates: 6,12,18   ; Dilated rates for each dilated convolution in DPN for validation
output_stride: 16   ; the output stride for validation

[test]
eval_crop_size: 513,513  ; the image crop size for testing
eval_scales: 1.0    ; single-scale or multiple-scale for testing
atrous_rates: 6,12,18  ; Dilated rates for each dilated convolution in DPN for testing
output_stride: 16   ; the output stride for testing
;num_plots_to_save: 5   ; The number of plots to visualize and to saved during testing. Comment this line to disable this function.
